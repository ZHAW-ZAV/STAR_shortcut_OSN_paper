{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential features: \n",
    "\n",
    "- Aircraft typecode\n",
    "- Mean descent rate of the STAR\n",
    "- Runway length\n",
    "- Runway orientation\n",
    "- obstructions near airport (boolean to indicate if there are mountains / buildings)\n",
    "- Runway configuration (parallel, single)\n",
    "- Weather visibility (fog / clear)\n",
    "- Wind speed\n",
    "- Wind direction (relative to the runway maybe, or something like that)\n",
    "- Storm presence\n",
    "- Traffic congestion (like number of aircraft in-flight at the same time)\n",
    "- STAR Type (ILS VS RNAV)\n",
    "- STAR Complexity (like number of waypoints or number of turns)\n",
    "- Initial length of STAR\n",
    "- Time of the day (night VS day)\n",
    "- Season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "#### EDDM\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\kruu\\\\store\\\\\"\n",
    "\n",
    "data_EDDM = pd.read_parquet(os.path.join(file_path + \"\\\\data_EDDM\\\\landing_df_EDDM.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Typecode selection\n",
    "\n",
    "typecode_list = data_EDDM.typecode.value_counts()[data_EDDM.typecode.value_counts() > 400].index.tolist()\n",
    "data_EDDM_reduced = data_EDDM.query(f\"typecode in {typecode_list}\")\n",
    "\n",
    "print(f\"typecode proportion: {len(typecode_list) / data_EDDM.typecode.nunique()}\")\n",
    "print(f\"Flight proportion: {len(data_EDDM_reduced) / len(data_EDDM)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flight airborne\n",
    "\n",
    "start_matrix = data_EDDM_reduced['start'].values[:, np.newaxis]\n",
    "stop_matrix = data_EDDM_reduced['stop'].values[:, np.newaxis]\n",
    "\n",
    "# Find overlaps: (start1 <= stop2) & (stop1 >= start2)\n",
    "overlap_matrix = (start_matrix <= stop_matrix.T) & (stop_matrix >= start_matrix.T)\n",
    "\n",
    "# Count the number of overlaps for each row\n",
    "data_EDDM_reduced['nb_aircraft'] = overlap_matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aircraft bodytype\n",
    "\n",
    "ac_body = {\n",
    "    \"Widebody\": [\"B763\", \"A333\"],\n",
    "    \"Narrowbody\": [\"A320\", \"BCS3\", \"A20N\", \"A319\", \"BCS1\", \"A21N\", \"B738\", \"A321\"],\n",
    "    \"Regional Jet\": [\"E190\", \"CRJ9\", \"E195\", \"DH8D\"],\n",
    "    \"Business jets\": [\"PC12\", \"C56X\", \"F2TH\", \"PC24\", \"C68A\", \"E55P\"],\n",
    "}\n",
    "\n",
    "def body_type(typecode, ac_body_dict):\n",
    "    for key, value in ac_body_dict.items():\n",
    "        if typecode in value:\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "data_EDDM_reduced[[\"body_type\"]] =  data_EDDM_reduced.typecode.apply(lambda x: pd.Series(body_type(x, ac_body)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time data\n",
    "import pytz\n",
    "\n",
    "data_EDDM_reduced[\"month\"] = pd.DatetimeIndex(data_EDDM_reduced.start).month.astype('category')\n",
    "data_EDDM_reduced[\"hour\"] = pd.DatetimeIndex(data_EDDM_reduced.start).hour.astype('category')\n",
    "data_EDDM_reduced[\"day\"] = pd.DatetimeIndex(data_EDDM_reduced.start).day.astype('category')\n",
    "data_EDDM_reduced[\"weekday\"] = pd.DatetimeIndex(data_EDDM_reduced.start).weekday.astype('category')\n",
    "\n",
    "def get_season(date):\n",
    "    # Extract month and day\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    # Define the seasons based on month and day\n",
    "    if (month == 12 and day >= 21) or (month in [1, 2]) or (month == 3 and day < 20):\n",
    "        return 'Winter'\n",
    "    elif (month == 3 and day >= 20) or (month in [4, 5]) or (month == 6 and day < 21):\n",
    "        return 'Spring'\n",
    "    elif (month == 6 and day >= 21) or (month in [7, 8]) or (month == 9 and day < 23):\n",
    "        return 'Summer'\n",
    "    elif (month == 9 and day >= 23) or (month in [10, 11]) or (month == 12 and day < 21):\n",
    "        return 'Fall'\n",
    "    \n",
    "data_EDDM_reduced['season'] = data_EDDM_reduced['start'].apply(get_season)\n",
    "\n",
    "def is_rush_hour(date): #based on hourly count bar plot: data_EDDM_reduced.groupby(\"hour\").id.count().plot(kind=\"bar\")\n",
    "    munich_tz = pytz.timezone('Europe/Munich')\n",
    "    \n",
    "    # Extract hour\n",
    "    hour = date.tz_convert(munich_tz).hour\n",
    "    minute = date.tz_convert(munich_tz).minute\n",
    "    time_in_minutes = hour * 60 + minute\n",
    "    \n",
    "    if (5 * 60 <= time_in_minutes <= 6 * 60) or \\\n",
    "       (7 * 60 <= time_in_minutes <= 9 * 60) or \\\n",
    "       (11 * 60 <= time_in_minutes <= 14 * 60) or \\\n",
    "       (16 * 60 <= time_in_minutes <= 20 * 60):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "data_EDDM_reduced['rush_hour'] = data_EDDM_reduced[\"start\"].apply(is_rush_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal distance proportion\n",
    "\n",
    "star_len_eddm = {\n",
    "    \"NAPS1B\": 61.9,\n",
    "    \"LAND1B\": 74.8,\n",
    "    \"ROKI1A\": 52.3,\n",
    "    \"BETO1A\": 61.3,\n",
    "}\n",
    "\n",
    "data_EDDM_reduced[\"nominal_distance\"] = data_EDDM_reduced.star.apply(lambda x: star_len_eddm[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meteo Data\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import RequestException, Timeout\n",
    "\n",
    "import logging\n",
    "import tqdm\n",
    "import time\n",
    "from traffic.data.weather import metar\n",
    "\n",
    "file_path_save = \"C:\\\\Users\\\\kruu\\\\store\\\\data_EDDM\"\n",
    "\n",
    "def get_meteo_data(row, airport):\n",
    "    start = row.start\n",
    "    stop = row.stop\n",
    "    \n",
    "    meteo = metar.METAR(airport).get(\n",
    "        start = start,\n",
    "        stop = stop,\n",
    "    )\n",
    "    \n",
    "    def safe_mean(values):\n",
    "        valid_values = [val.value() for val in values if val is not None]\n",
    "        return np.mean(valid_values) if valid_values else None  # Return None if no valid values\n",
    "    \n",
    "    wind_dir = safe_mean(meteo.wind_dir.values)\n",
    "    wind_speed = safe_mean(meteo.wind_speed.values)\n",
    "    vis = safe_mean(meteo.vis.values) / 1000 # In km\n",
    "    temp = safe_mean(meteo.temp.values)\n",
    "    press = safe_mean(meteo.press.values) - 1013 # relative to standard pressure\n",
    "    \n",
    "    return wind_dir, wind_speed, vis, temp, press\n",
    "\n",
    "def get_meteo_data_with_retry(row, airport, max_retries=3, delay=5):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return get_meteo_data(row, airport)\n",
    "        except RequestException as e:\n",
    "            retries += 1\n",
    "            print(f\"Error: {e}, retrying ({retries}/{max_retries})...\")\n",
    "    return None, None, None, None, None  # If all retries fail, return None values\n",
    "\n",
    "batch_size = 2000  # Smaller batch size\n",
    "num_batches = len(data_EDDM_reduced) // batch_size + 1\n",
    "\n",
    "for i in tqdm.tqdm(range(num_batches)):\n",
    "    if not os.path.exists(os.path.join(file_path_save + f\"landing_df_EDDM_with_meteo_{i}_of_{num_batches-1}.parquet\")):\n",
    "        batch = data_EDDM_reduced.iloc[i * batch_size:(i + 1) * batch_size]\n",
    "        batch[[\"avg_wind_dir\", \"avg_wind_speed\", \"avg_vis\", \"avg_temp\", \"avg_press\"]] = batch.apply(\n",
    "            lambda row: pd.Series(get_meteo_data_with_retry(row, \"EDDM\")), axis=1\n",
    "        )\n",
    "        batch.to_parquet(os.path.join(file_path_save + f\"landing_df_EDDM_with_meteo_{i}_of_{num_batches-1}.parquet\"))\n",
    "    else:\n",
    "        print(\"file already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "### LIRF\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\kruu\\\\store\\\\\"\n",
    "\n",
    "data_LIRF = pd.read_parquet(os.path.join(file_path + \"\\\\data_LIRF\\\\landing_df_LIRF.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Typecode selection\n",
    "\n",
    "typecode_list = data_LIRF.typecode.value_counts()[data_LIRF.typecode.value_counts() > 400].index.tolist()\n",
    "data_LIRF_reduced = data_LIRF.query(f\"typecode in {typecode_list}\")\n",
    "\n",
    "print(f\"typecode proportion: {len(typecode_list) / data_LIRF.typecode.nunique()}\")\n",
    "print(f\"Flight proportion: {len(data_LIRF_reduced) / len(data_LIRF)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flight airborne\n",
    "\n",
    "start_matrix = data_LIRF_reduced['start'].values[:, np.newaxis]\n",
    "stop_matrix = data_LIRF_reduced['stop'].values[:, np.newaxis]\n",
    "\n",
    "# Find overlaps: (start1 <= stop2) & (stop1 >= start2)\n",
    "overlap_matrix = (start_matrix <= stop_matrix.T) & (stop_matrix >= start_matrix.T)\n",
    "\n",
    "# Count the number of overlaps for each row\n",
    "data_LIRF_reduced['nb_aircraft'] = overlap_matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aircraft bodytype\n",
    "\n",
    "ac_body = {\n",
    "    \"Widebody\": [\"B763\", \"A333\", \"B789\", \"A332\", \"A333\", \"B772\"],\n",
    "    \"Narrowbody\": [\"A320\", \"BCS3\", \"A20N\", \"A319\", \"BCS1\", \"A21N\", \"B738\", \"A321\", \"A20N\", \"A21N\"],\n",
    "    \"Regional Jet\": [\"E190\", \"CRJ9\", \"E195\", \"DH8D\"],\n",
    "    \"Business jets\": [\"PC12\", \"C56X\", \"F2TH\", \"PC24\", \"C68A\", \"E55P\"],\n",
    "}\n",
    "\n",
    "def body_type(typecode, ac_body_dict):\n",
    "    for key, value in ac_body_dict.items():\n",
    "        if typecode in value:\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "data_LIRF_reduced[[\"body_type\"]] =  data_LIRF_reduced.typecode.apply(lambda x: pd.Series(body_type(x, ac_body)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time data\n",
    "import pytz\n",
    "\n",
    "data_LIRF_reduced[\"month\"] = pd.DatetimeIndex(data_LIRF_reduced.start).month.astype('category')\n",
    "data_LIRF_reduced[\"hour\"] = pd.DatetimeIndex(data_LIRF_reduced.start).hour.astype('category')\n",
    "data_LIRF_reduced[\"day\"] = pd.DatetimeIndex(data_LIRF_reduced.start).day.astype('category')\n",
    "data_LIRF_reduced[\"weekday\"] = pd.DatetimeIndex(data_LIRF_reduced.start).weekday.astype('category')\n",
    "\n",
    "def get_season(date):\n",
    "    # Extract month and day\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    # Define the seasons based on month and day\n",
    "    if (month == 12 and day >= 21) or (month in [1, 2]) or (month == 3 and day < 20):\n",
    "        return 'Winter'\n",
    "    elif (month == 3 and day >= 20) or (month in [4, 5]) or (month == 6 and day < 21):\n",
    "        return 'Spring'\n",
    "    elif (month == 6 and day >= 21) or (month in [7, 8]) or (month == 9 and day < 23):\n",
    "        return 'Summer'\n",
    "    elif (month == 9 and day >= 23) or (month in [10, 11]) or (month == 12 and day < 21):\n",
    "        return 'Fall'\n",
    "    \n",
    "data_LIRF_reduced['season'] = data_LIRF_reduced['start'].apply(get_season)\n",
    "\n",
    "def is_rush_hour(date):\n",
    "    rome_tz = pytz.timezone('Europe/Rome')\n",
    "    \n",
    "    # Extract hour\n",
    "    hour = date.tz_convert(rome_tz).hour\n",
    "    minute = date.tz_convert(rome_tz).minute\n",
    "    time_in_minutes = hour * 60 + minute\n",
    "    \n",
    "    if (6 * 60 + 30 <= time_in_minutes <= 9 * 60) or \\\n",
    "       (11 * 60 <= time_in_minutes <= 14 * 60) or \\\n",
    "       (16 * 60 + 30 <= time_in_minutes <= 18 * 60) or \\\n",
    "       (19 * 60 <= time_in_minutes <= 21 * 60):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "data_LIRF_reduced['rush_hour'] = data_LIRF_reduced[\"start\"].apply(is_rush_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal distance proportion\n",
    "\n",
    "star_len_eddm = {\n",
    "    \"ELKA2A\": 132.7,\n",
    "    \"VALM2C\": 92.7,\n",
    "    \"RITE2A\": 94.8,\n",
    "    \"LAT2C\": 74.3,\n",
    "}\n",
    "\n",
    "data_LIRF_reduced[\"nominal_distance\"] = data_LIRF_reduced.star.apply(lambda x: star_len_eddm[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meteo Data\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import RequestException, Timeout\n",
    "\n",
    "import logging\n",
    "import tqdm\n",
    "import time\n",
    "from traffic.data.weather import metar\n",
    "\n",
    "file_path_save = \"C:\\\\Users\\\\kruu\\\\store\\\\data_LIRF\\\\\"\n",
    "\n",
    "def get_meteo_data(row, airport):\n",
    "    start = row.start\n",
    "    stop = row.stop\n",
    "    \n",
    "    try:\n",
    "        # Attempt to retrieve the METAR data\n",
    "        meteo = metar.METAR(airport).get(\n",
    "            start=start,\n",
    "            stop=stop,\n",
    "        )\n",
    "    except KeyError:\n",
    "        print(f\"KeyError for airport {airport}, start {start}, stop {stop}\")\n",
    "        return None, None, None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    def safe_mean(values):\n",
    "        valid_values = [val.value() for val in values if val is not None]\n",
    "        return np.mean(valid_values) if valid_values else None  # Return None if no valid values\n",
    "    \n",
    "    wind_dir = safe_mean(meteo.wind_dir.values)\n",
    "    wind_speed = safe_mean(meteo.wind_speed.values)\n",
    "    vis = safe_mean(meteo.vis.values) / 1000 # In km\n",
    "    temp = safe_mean(meteo.temp.values)\n",
    "    press = safe_mean(meteo.press.values) - 1013 # relative to standard pressure\n",
    "    \n",
    "    return wind_dir, wind_speed, vis, temp, press\n",
    "\n",
    "\n",
    "batch_size = 1000  # Smaller batch size\n",
    "num_batches = len(data_LIRF_reduced) // batch_size + 1\n",
    "\n",
    "for i in range(num_batches):\n",
    "    if os.path.exists(os.path.join(file_path_save + f\"landing_df_LIRF_with_meteo_{i}_of_{num_batches}.parquet\")):\n",
    "        print(\"file already exists\")\n",
    "    else:\n",
    "        batch = data_LIRF_reduced.iloc[i * batch_size:(i + 1) * batch_size]\n",
    "        batch[[\"avg_wind_dir\", \"avg_wind_speed\", \"avg_vis\", \"avg_temp\", \"avg_press\"]] = batch.apply(\n",
    "            lambda row: pd.Series(get_meteo_data(row, \"LIRF\")), axis=1\n",
    "        )\n",
    "        batch.to_parquet(os.path.join(file_path_save + f\"landing_df_LIRF_with_meteo_{i}_of_{num_batches}.parquet\"))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "### LSGG\n",
    "****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\kruu\\\\store\\\\\"\n",
    "\n",
    "data_LSGG = pd.read_parquet(os.path.join(file_path + \"landing_df_LSGG.parquet\"))\n",
    "data_LSGG[\"airport\"] = \"LSGG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nominal STAR distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_LSGG = data_LSGG.query(\"typecode.isna() ==  False\")\n",
    "data_LSGG.runway = data_LSGG.runway.apply(lambda x: x[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAR entry point + STAR IAF + runway + initial length in NM\n",
    "star_len_lsgg = {\n",
    "    \"AKIT2N\": [\"AKITO\", \"INDIS\", 95.2],\n",
    "    \"AKIT3R\": [\"AKITO\", \"SPR\", 91.6],\n",
    "    \"BANK3N\": [\"BANKO\", \"INDIS\", 67.2],\n",
    "    \"BANK3R\": [\"BANKO\", \"SPR\", 63.6],\n",
    "    \"BELU3N\": [\"BELUS\", \"INDIS\", 75.4],\n",
    "    \"BELU3R\": [\"BELUS\", \"SPR\", 65.5],\n",
    "    \"BENO1N\": [\"BENOT\", \"INDIS\", 90.1], # almost same distance as BENO1P / same entry + exit points\n",
    "    \"BENO1R\": [\"BENOT\", \"SPR\", 47.5], # almost same distance as BENO1T / same entry + exit points\n",
    "    \"DJL2N\": [\"DJL\", \"INDIS\", 94.9],\n",
    "    \"DJL2R\": [\"DJL\", \"SPR\", 91.6],\n",
    "    \"KINE2N\": [\"KINES\", \"INDIS\", 86.4],\n",
    "    \"KINE2R\": [\"KINES\", \"SPR\", 82.8],\n",
    "    \"LUSA2N\": [\"LUSAR\", \"INDIS\", 70.5],\n",
    "    \"LUSA2R\": [\"LUSAR\", \"SPR\", 67.2],\n",
    "    \"ULME1N\": [\"ULMES\", \"INDIS\", 90.7], # almost same distance as BENO1P / same entry + exit points\n",
    "    \"ULME1R\": [\"ULMES\", \"SPR\", 45.5],\n",
    "}\n",
    "\n",
    "\n",
    "#APP len depending on IAF and runway\n",
    "app_len_lsgg ={\n",
    "    (\"SPR\", \"22\") : 18.7,\n",
    "    (\"INDIS\", \"04\") : 17.4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_star(row, star_dict, app_dict):\n",
    "    navaid_iaf_tuple = (row['navaid'], row['iaf'])\n",
    "    for key, value in star_dict.items():\n",
    "        if tuple(value[:2]) == navaid_iaf_tuple:\n",
    "            return key, value[2] + app_dict[(row['iaf'], row['runway'])]\n",
    "    return None, None\n",
    "\n",
    "data_LSGG[\"iaf\"] = data_LSGG.runway.apply(lambda x: \"SPR\" if x[:2] == \"22\" else \"INDIS\")\n",
    "data_LSGG[[\"star\", \"nominal_distance\"]] =  data_LSGG.apply(lambda row: pd.Series(find_star(row, star_len_lsgg, app_len_lsgg)), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traffic congestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_matrix = data_LSGG['start'].values[:, np.newaxis]\n",
    "stop_matrix = data_LSGG['stop'].values[:, np.newaxis]\n",
    "\n",
    "# Find overlaps: (start1 <= stop2) & (stop1 >= start2)\n",
    "overlap_matrix = (start_matrix <= stop_matrix.T) & (stop_matrix >= start_matrix.T)\n",
    "\n",
    "# Count the number of overlaps for each row\n",
    "data_LSGG['nb_aircraft'] = overlap_matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aircraft type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typecode_list = data_LSGG.typecode.value_counts()[data_LSGG.typecode.value_counts() > 400].index.tolist()\n",
    "data_LSGG_reduced = data_LSGG.query(f\"typecode in {typecode_list}\")\n",
    "\n",
    "print(f\"typecode proportion: {len(typecode_list) / data_LSGG.typecode.nunique()}\")\n",
    "print(f\"Flight proportion: {len(data_LSGG_reduced) / len(data_LSGG)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight category for the typecodes that appear more than 400 times per year\n",
    "\n",
    "wtc_lsgg = {\n",
    "    \"H\": [\"B763\", \"A333\"],\n",
    "    \"J\": [],\n",
    "    \"L\": [\"PC12\", ],\n",
    "    \"L/M\": [],\n",
    "    \"M\": [\"A320\", \"BCS3\", \"A20N\", \"A319\", \"BCS1\", \"E190\", \"B738\", \"CRJ9\", \"E55P\", \"A21N\", \"E195\", \"C56X\", \"A321\", \"F2TH\", \"PC24\", \"DH8D\", \"C68A\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw time data\n",
    "\n",
    "data_LSGG_reduced[\"month\"] = pd.DatetimeIndex(data_LSGG_reduced.start).month.astype('category')\n",
    "data_LSGG_reduced[\"hour\"] = pd.DatetimeIndex(data_LSGG_reduced.start).hour.astype('category')\n",
    "data_LSGG_reduced[\"day\"] = pd.DatetimeIndex(data_LSGG_reduced.start).day.astype('category')\n",
    "data_LSGG_reduced[\"weekday\"] = pd.DatetimeIndex(data_LSGG_reduced.start).weekday.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the season based on the date\n",
    "\n",
    "def get_season(date):\n",
    "    # Extract month and day\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    # Define the seasons based on month and day\n",
    "    if (month == 12 and day >= 21) or (month in [1, 2]) or (month == 3 and day < 20):\n",
    "        return 'Winter'\n",
    "    elif (month == 3 and day >= 20) or (month in [4, 5]) or (month == 6 and day < 21):\n",
    "        return 'Spring'\n",
    "    elif (month == 6 and day >= 21) or (month in [7, 8]) or (month == 9 and day < 23):\n",
    "        return 'Summer'\n",
    "    elif (month == 9 and day >= 23) or (month in [10, 11]) or (month == 12 and day < 21):\n",
    "        return 'Fall'\n",
    "    \n",
    "data_LSGG_reduced['season'] = data_LSGG_reduced['start'].apply(get_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "# Rush hour (I've taken Rome rush hours here, maybe it's different in Geneva)\n",
    "\n",
    "def is_rush_hour(date):\n",
    "    \n",
    "    geneva_tz = pytz.timezone('Europe/Zurich')\n",
    "    \n",
    "    # Extract hour\n",
    "    hour = date.tz_convert(geneva_tz).hour\n",
    "    minute = date.tz_convert(geneva_tz).minute\n",
    "    time_in_minutes = hour * 60 + minute\n",
    "    \n",
    "    if (6 * 60 + 30 <= time_in_minutes <= 9 * 60) or \\\n",
    "       (11 * 60 <= time_in_minutes <= 14 * 60) or \\\n",
    "       (16 * 60 + 30 <= time_in_minutes <= 18 * 60) or \\\n",
    "       (19 * 60 <= time_in_minutes <= 21 * 60):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "data_LSGG_reduced['rush_hour'] = data_LSGG_reduced[\"start\"].apply(is_rush_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Meteo Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import RequestException, Timeout\n",
    "\n",
    "import logging\n",
    "import tqdm\n",
    "import time\n",
    "from traffic.data.weather import metar\n",
    "\n",
    "file_path_save = \"C:\\\\Users\\\\kruu\\\\store\\\\\"\n",
    "\n",
    "def get_meteo_data(row, airport):\n",
    "    start = row.start\n",
    "    stop = row.stop\n",
    "    \n",
    "    meteo = metar.METAR(airport).get(\n",
    "        start = start,\n",
    "        stop = stop,\n",
    "    )\n",
    "    \n",
    "    def safe_mean(values):\n",
    "        valid_values = [val.value() for val in values if val is not None]\n",
    "        return np.mean(valid_values) if valid_values else None  # Return None if no valid values\n",
    "    \n",
    "    wind_dir = safe_mean(meteo.wind_dir.values)\n",
    "    wind_speed = safe_mean(meteo.wind_speed.values)\n",
    "    vis = safe_mean(meteo.vis.values) / 1000 # In km\n",
    "    temp = safe_mean(meteo.temp.values)\n",
    "    press = safe_mean(meteo.press.values) - 1013 # relative to standard pressure\n",
    "    \n",
    "    return wind_dir, wind_speed, vis, temp, press\n",
    "\n",
    "def get_meteo_data_with_retry(row, airport, max_retries=3, delay=5):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return get_meteo_data(row, airport)\n",
    "        except RequestException as e:\n",
    "            retries += 1\n",
    "            print(f\"Error: {e}, retrying ({retries}/{max_retries})...\")\n",
    "            # time.sleep(delay)\n",
    "    return None, None, None, None, None  # If all retries fail, return None values\n",
    "\n",
    "batch_size = 2000  # Smaller batch size\n",
    "num_batches = len(data_LSGG_reduced) // batch_size + 1\n",
    "\n",
    "for i in tqdm.tqdm(range(num_batches)):\n",
    "    if not os.path.exists(os.path.join(file_path + f\"landing_df_LSGG_with_meteo_{i}_of_{num_batches}.parquet\")):\n",
    "        batch = data_LSGG_reduced.iloc[i * batch_size:(i + 1) * batch_size]\n",
    "        batch[[\"avg_wind_dir\", \"avg_wind_speed\", \"avg_vis\", \"avg_temp\", \"avg_press\"]] = batch.apply(\n",
    "            lambda row: pd.Series(get_meteo_data_with_retry(row, \"LSGG\")), axis=1\n",
    "        )\n",
    "        batch.to_parquet(os.path.join(file_path + f\"landing_df_LSGG_with_meteo_{i}_of_{num_batches}.parquet\"))\n",
    "    else:\n",
    "        print(\"file already exists\")\n",
    "    # time.sleep(10)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
